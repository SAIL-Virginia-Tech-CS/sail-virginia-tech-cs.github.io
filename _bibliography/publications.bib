@article{rho_EscalatedPoliceStops_2023,
  title = {Escalated Police Stops of {{Black}} Men Are Linguistically and Psychologically Distinct in Their Earliest Moments},
  author = {Rho, Eugenia H. and Harrington, Maggie and Zhong, Yuyang and Pryzant, Reid and Camp, Nicholas P. and Jurafsky, Dan and Eberhardt, Jennifer L.},
  date = {2023-06-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {23},
  pages = {e2216162120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2216162120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2216162120},
  urldate = {2023-09-20},
  abstract = {Across the United States, police chiefs, city officials, and community leaders alike have highlighted the need to de-escalate police encounters with the public. This concern about escalation extends from encounters involving use of force to routine car stops, where Black drivers are disproportionately pulled over. Yet, despite the calls for action, we know little about the trajectory of police stops or how escalation unfolds. In study 1, we use methods from computational linguistics to analyze police body-worn camera footage from 577 stops of Black drivers. We find that stops with escalated outcomes (those ending in arrest, handcuffing, or a search) diverge from stops without these outcomes in their earliest moments—even in the first 45 words spoken by the officer. In stops that result in escalation, officers are more likely to issue commands as their opening words to the driver and less likely to tell drivers the reason why they are being stopped. In study 2, we expose Black males to audio clips of the same stops and find differences in how escalated stops are perceived: Participants report more negative emotion, appraise officers more negatively, worry about force being used, and predict worse outcomes after hearing only the officer’s initial words in escalated versus non-escalated stops. Our findings show that car stops that end in escalated outcomes sometimes begin in an escalated fashion, with adverse effects for Black male drivers and, in turn, police–community relations.},
  file = {https://www.pnas.org/doi/epdf/10.1073/pnas.2216162120},
  preview = {police.png}
}

@online{gunturi_ToxVisEnablingInterpretability_2023,
  title = {{{ToxVis}}: {{Enabling Interpretability}} of {{Implicit}} vs. {{Explicit Toxicity Detection Models}} with {{Interactive Visualization}}},
  shorttitle = {{{ToxVis}}},
  author = {Gunturi, Uma and Ding, Xiaohan and Rho, Eugenia H.},
  date = {2023-03-01},
  eprint = {2303.09402},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.09402},
  url = {http://arxiv.org/abs/2303.09402},
  urldate = {2023-09-20},
  abstract = {The rise of hate speech on online platforms has led to an urgent need for effective content moderation. However, the subjective and multi-faceted nature of hateful online content, including implicit hate speech, poses significant challenges to human moderators and content moderation systems. To address this issue, we developed ToxVis, a visually interactive and explainable tool for classifying hate speech into three categories: implicit, explicit, and non-hateful. We fine-tuned two transformer-based models using RoBERTa, XLNET, and GPT-3 and used deep learning interpretation techniques to provide explanations for the classification results. ToxVis enables users to input potentially hateful text and receive a classification result along with a visual explanation of which words contributed most to the decision. By making the classification process explainable, ToxVis provides a valuable tool for understanding the nuances of hateful content and supporting more effective content moderation. Our research contributes to the growing body of work aimed at mitigating the harms caused by online hate speech and demonstrates the potential for combining state-of-the-art natural language processing models with interpretable deep learning techniques to address this critical issue. Finally, ToxVis can serve as a resource for content moderators, social media platforms, and researchers working to combat the spread of hate speech online.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {http://arxiv.org/abs/2303.09402.pdf},
  preview = {toxic.png}
}

@online{ding_SameWordsDifferent_2023,
  title = {Same {{Words}}, {{Different Meanings}}: {{Semantic Polarization}} in {{Broadcast Media Language Forecasts Polarization}} on {{Social Media Discourse}}},
  shorttitle = {Same {{Words}}, {{Different Meanings}}},
  author = {Ding, Xiaohan and Horning, Mike and Rho, Eugenia H.},
  date = {2023-01-24},
  eprint = {2301.08832},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.08832},
  urldate = {2023-09-20},
  abstract = {With the growth of online news over the past decade, empirical studies on political discourse and news consumption have focused on the phenomenon of filter bubbles and echo chambers. Yet recently, scholars have revealed limited evidence around the impact of such phenomenon, leading some to argue that partisan segregation across news audiences cannot be fully explained by online news consumption alone and that the role of traditional legacy media may be as salient in polarizing public discourse around current events. In this work, we expand the scope of analysis to include both online and more traditional media by investigating the relationship between broadcast news media language and social media discourse. By analyzing a decade's worth of closed captions (2 million speaker turns) from CNN and Fox News along with topically corresponding discourse from Twitter, we provide a novel framework for measuring semantic polarization between America's two major broadcast networks to demonstrate how semantic polarization between these outlets has evolved (Study 1), peaked (Study 2) and influenced partisan discussions on Twitter (Study 3) across the last decade. Our results demonstrate a sharp increase in polarization in how topically important keywords are discussed between the two channels, especially after 2016, with overall highest peaks occurring in 2020. The two stations discuss identical topics in drastically distinct contexts in 2020, to the extent that there is barely any linguistic overlap in how identical keywords are contextually discussed. Further, we demonstrate at scale, how such partisan division in broadcast media language significantly shapes semantic polarity trends on Twitter (and vice-versa), empirically linking for the first time, how online discussions are influenced by televised media.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Social and Information Networks},
  file = {/home/lance/Documents/Lance's Vault/attachments/articles/ding_SameWordsDifferent_2023.pdf;/home/lance/Zotero/storage/M3AWTFRF/2301.html},
  preview = {different.png}
}
